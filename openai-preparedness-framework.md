# OpenAI의 대비 프레임워크 v1 (Preparedness Framework v1)

OpenAI가 2024년 4월에 발표한 대비 프레임워크(Preparedness Framework) v1은 **최첨단 AI 모델(Frontier AI)**의 **심각하고 파국적인 위험(Catastrophic Risks)**을 선제적으로 평가하고 관리하기 위해 고안된 자체 규제 시스템입니다. 이 프레임워크는 AI 모델이 잠재적으로 위험한 능력(Dangerous Capabilities)을 획득하는 것을 사전에 감지하고, 해당 모델의 개발 및 배포를 안전하게 통제하는 것을 목표로 합니다. 🛡️

## 1. 프레임워크의 4가지 핵심 단계

대비 프레임워크는 AI 모델의 전체 수명 주기(모델 설계부터 배포 후 모니터링까지)에 걸쳐 위험을 관리하는 반복적인 사이클로 구성됩니다.

| 단계 | 목표 | 주요 활동 |
|------|------|-----------|
| **1. 위험 인텔리전스 (Risk Intelligence)** | 잠재적인 파국적 위험을 식별하고, 해당 위험의 심각도를 평가합니다. | 전문가 분석, 문헌 검토, 시나리오 계획(예: 사이버/CBRN 위험), AI 시스템 취약점 연구를 통해 위험 목록을 작성하고 업데이트합니다. |
| **2. 임계값 설정 (Threshold Determination)** | 모델의 능력과 위험 수준에 따라 **허용 가능한 배포 기준(Acceptable Deployment Criteria)**을 정의합니다. | 모델이 획득해서는 안 되는 위험한 능력의 임계값(Threshold)을 설정하고, 이를 초과할 경우 취해야 할 **완화 조치(Mitigation)**를 미리 정합니다. |
| **3. 평가 (Evaluation)** | 개발 중인 모델이 설정된 위험 임계값에 도달했는지 객관적으로 측정합니다. | 레드팀 구성(Red Teaming), 표준화된 벤치마크 테스트, 전문가 주도의 모의 공격을 통해 모델의 안전성, 통제 가능성, 위험한 능력 보유 여부를 평가합니다. |
| **4. 완화 및 통제 (Mitigation & Governance)** | 평가 결과 임계값을 초과한 경우, 모델의 배포를 안전하게 통제하고 위험을 줄이기 위한 조치를 실행합니다. | 배포 일시 중지(Pausing), 접근 제한(Limited Access), 위험 기능을 제거하기 위한 재정렬(Re-alignment), 안전 장치(Safeguards) 강화 등의 조치를 취합니다. |

## 2. 파국적 위험 영역 (Catastrophic Risk Domains)

OpenAI는 이 프레임워크를 통해 관리해야 할 파국적 위험의 주요 영역을 정의하고, 각 영역에서 모델이 위험한 능력을 획득했는지 측정합니다.

| 위험 영역 | 설명 및 주요 평가 대상 |
|-----------|----------------------|
| **자율성 및 복제 (Autonomy & Replication)** | AI 모델이 외부 환경과 상호작용하며 자율적으로 목표를 달성하고, 스스로를 복제하거나 개선할 수 있는 능력. (통제력 상실 위험) |
| **CBRN (화학, 생물학, 방사능, 핵)** | AI 모델이 위험 물질에 대한 지식을 획득하고, 이를 제조하거나 확산하는 데 기여할 수 있는 능력. |
| **사이버 보안 (Cybersecurity)** | AI 모델이 스스로 복잡한 사이버 공격을 계획, 실행하거나, 소프트웨어의 취약점을 탐색 및 악용할 수 있는 능력. |
| **모델 통제력 상실 (Model Control Failure)** | AI 모델이 악의적인 유저의 조작(예: Jailbreak) 없이도 예상치 못한 방식으로 작동하거나, 개발자의 통제를 벗어나는 행위를 하는 경우. |

## 3. 임계값 및 위험 등급

OpenAI는 프레임워크를 통해 모델의 위험 수준을 4가지 등급으로 나누고, 각 등급에 따라 다른 조치를 적용합니다.

**위험 등급 (Risk Tiers):**
- Tier 0: 무시할 수 있는 위험 (Ignorable)
- Tier 1: 낮은 위험 (Low Risk)
- Tier 2: 중간 위험 (Medium Risk)
- Tier 3: 높은 위험 (High Risk)
- Tier 4: 치명적 위험 (Severe Risk)

**핵심 원칙:** 모델이 Tier 3 (높은 위험) 또는 Tier 4 (치명적 위험) 임계값에 도달하거나 초과하는 것으로 평가될 경우, OpenAI는 해당 모델의 배포를 중지하거나 접근을 크게 제한하는 등 강력한 완화 조치를 취해야 합니다.

이 프레임워크는 AI 안전에 대한 선제적인 책임을 강조하며, AI 법(AI Act)이나 NIST RMF와 같은 외부 규제와 함께 업계 자체의 안전 기준을 제시하는 중요한 역할을 합니다.

## 키워드
- OpenAI
- 대비 프레임워크
- Preparedness Framework
- 최첨단 AI
- Frontier AI
- 파국적 위험
- Catastrophic Risks
- 위험한 능력
- Dangerous Capabilities
- 위험 인텔리전스
- 임계값 설정
- 모델 평가
- 레드팀
- Red Teaming
- 완화 조치
- 모델 통제
- 자율성
- 복제
- CBRN
- 사이버 보안
- 모델 통제력 상실
- 위험 등급
- Tier 0
- Tier 1
- Tier 2
- Tier 3
- Tier 4
- 높은 위험
- 치명적 위험
- 안전 장치
- Safeguards
- 재정렬
- Re-alignment
- 접근 제한
- 배포 일시 중지
- AI 안전
- AI 윤리
- AI 거버넌스
- AI 위험 관리
- AI 수명 주기
- 통제력 상실
- Jailbreak
- 위험 감지