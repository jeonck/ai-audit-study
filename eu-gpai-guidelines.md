# EU 일반 목적 인공지능(GPAI) 실행 규약: 위험관리

유럽연합(EU)의 **'일반 목적 인공지능(GPAI) 실행 규약(Code of Practice)'**에서 위험관리(Risk Management)는 '안전 및 보안(Safety and Security)' 장(Chapter)에 포함되어 있으며, 이는 특히 **시스템적 위험(Systemic Risk)**을 수반하는 가장 발전된 GPAI 모델 제공자에게 적용되는 핵심적인 의무 사항입니다. 🚨

이 실행 규약은 **EU 인공지능 법(AI Act)**의 GPAI 관련 의무 사항 준수를 돕기 위한 자발적 도구이지만, 준수 시 법적 확실성을 높여줍니다.

## GPAI 실행 규약의 위험관리 주요 내용 (안전 및 보안 장)

위험관리의 핵심 목표는 GPAI 모델이 초래할 수 있는 **심각한 부작용(Harmful Effects)**과 시스템적 위험을 식별, 평가, 완화하고 투명성을 확보하는 것입니다.

### 1. 위험관리 프레임워크 구축 및 이행

GPAI 모델 제공자는 다음을 포함하는 **강력한 안전 및 보안 프레임워크(Safety and Security Framework, SSF)**를 구축하고 이행해야 합니다.

- **시스템적 위험 식별, 평가 및 완화**: 모델 수명 주기 전체에 걸쳐 발생하는 시스템적 위험(예: 모델 통제력 상실, 심각한 유해 콘텐츠 생성, 생화학 무기 개발 촉진 등)을 사전에 평가하고 완화 조치를 취합니다.
- **사고 모니터링 및 보고**: 심각한 사고(Serious Incidents) 발생 시 이를 추적하고 EU 집행위원회(AI Office)에 보고하는 절차를 마련합니다.

### 2. 모델 평가 및 레드팀(Red Teaming)

위험관리의 핵심적인 기술적 방법으로, 모델의 안전성을 객관적으로 검증하는 활동이 요구됩니다.

- **모델 평가 (Model Evaluation)**: 모델 개발 수명 주기 전반에 걸쳐 시스템적 위험을 평가하기 위한 광범위한 평가 방법론을 사용합니다.
- **레드팀 구성 (Red Teaming)**: 모델이 악의적인 오용이나 취약점에 노출될 수 있는지를 식별하고 테스트하기 위해 독립적인 전문가 그룹(Red Team)을 구성하여 모의 공격을 수행합니다.

### 3. 투명성 및 문서화

위험관리 활동의 투명성을 확보하여 책임성을 높입니다.

- **모델 보고서 (Model Report) 발행**: 모델의 기능, 성능, 알려진 위험 및 이러한 위험을 완화하기 위해 취한 조치 등을 포함하는 기술 문서를 작성하고 유지합니다.
- **정보 공유**: 잠재적인 시스템적 위험에 대한 정보를 AI Office 및 다운스트림 제공자(모델을 기반으로 AI 시스템을 만드는 기업)와 공유할 수 있는 절차를 수립합니다.

### 4. 거버넌스 및 조직 관리

조직 내부적으로 위험을 관리할 수 있는 체계를 갖춥니다.

- **책임 할당**: 고위 경영진 및 이사회 수준에서 시스템적 위험에 대한 책임과 자원을 할당하고 감독합니다.
- **사이버 보안**: 모델 학습 및 배포 단계에서 최첨단 사이버 보안 조치를 적용하여 모델 자체와 기반 인프라를 보호합니다.

## 적용 대상

안전 및 보안 장의 위험관리 의무는 모든 GPAI 제공자에게 적용되는 것이 아니라, EU AI 법상 시스템적 위험을 수반하는 것으로 지정된 가장 발전된 GPAI 모델 제공자에게만 적용됩니다.

- 시스템적 위험 모델은 주로 학습에 사용된 누적 연산량이 기준 이상인 모델 등 (예: $10^{25}$ FLOPS)이거나, 그 외의 다른 기준(예: 광범위한 활용도)에 따라 지정될 수 있습니다.

이 실행 규약은 해당 사업자들이 AI Act의 구속력 있는 법적 요구 사항을 충족했음을 입증하는 실질적인 방법을 제공하며, 국제적으로 AI 안전 표준을 이끌어가는 역할을 할 것으로 기대됩니다.