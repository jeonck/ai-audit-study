# 전통적인 딥러닝 모델의 보안 위협

전통적인 딥러닝(Deep Learning) 모델이 직면하는 보안 위협은 주로 모델의 데이터, 구조, 배포 환경을 표적으로 하며, AI 시스템의 정확성, 신뢰성, 기밀성을 손상시키는 데 초점을 맞춥니다. 이는 일반적인 소프트웨어 보안 위협과는 구별되는, AI 모델 자체의 취약성을 악용하는 공격이 주를 이룹니다. 🔐

## 1. 모델 무결성 공격 (Integrity Attacks)

모델이 잘못된 예측이나 결정을 내리도록 유도하여 시스템의 신뢰성을 손상시키는 공격입니다.

### A. 적대적 공격 (Adversarial Attacks)

공격자가 모델을 속이기 위해 입력 데이터에 미세하고 사람이 인지하기 어려운 노이즈를 추가하는 방식입니다.

**위협 내용**: AI 시스템은 이 미세하게 변조된 입력을 정상적인 입력으로 인식하지 못하고, 완전히 잘못된 결과를 도출합니다. 예를 들어, 자율주행차의 표지판 인식 모델에 미세한 패치를 붙여 '정지 표지판'을 '속도 제한 표지판'으로 오인식하게 만들 수 있습니다.

**유형**:

- **회피 공격 (Evasion Attacks)**: 모델이 이미 훈련을 마친 후, 서비스 단계(Inference time)에서 잘못된 분류를 유도하는 공격입니다.

- **독성 공격 (Poisoning Attacks)**: 모델의 훈련 데이터에 독성 데이터를 주입하여 모델 자체에 백도어를 심거나 성능을 저하시키는 공격입니다.

### B. 데이터 포이즌 공격 (Data Poisoning Attacks)

모델이 훈련되는 과정에 악의적인 데이터를 주입하여 모델의 학습을 방해하거나, 특정 조건에서만 오작동하게 만드는 공격입니다.

**위협 내용**: 공격자가 훈련 데이터셋에 잘못된 레이블을 가진 데이터를 의도적으로 혼합합니다. 이는 모델의 전반적인 정확도를 저하시키거나, 특정 트리거가 발생했을 때만 모델이 오작동하도록 **백도어(Backdoor)**를 심는 데 사용될 수 있습니다.

## 2. 모델 기밀성 공격 (Confidentiality Attacks)

모델 자체가 가진 지적 재산(IP)이나 모델이 학습한 민감한 정보를 추출하려는 공격입니다.

### A. 모델 추출 공격 (Model Extraction / Stealing)

공격자가 대상 모델에 수많은 쿼리(Query)를 전송하고 그 응답을 분석하여, **원래 모델과 거의 동일한 기능을 수행하는 복제 모델(Surrogate Model)**을 만들어내는 공격입니다.

**위협 내용**: 공격자는 원본 모델에 접근하지 않고도 그 기능을 복제하여 지적 재산권을 침해하고, 서비스의 경쟁 우위를 약화시키며, 복제 모델을 이용해 **다른 공격(예: 적대적 공격)**을 개발하는 데 활용할 수 있습니다.

### B. 멤버십 추론 공격 (Membership Inference Attacks)

공격자가 모델의 응답을 분석하여, 특정 데이터 포인트가 모델의 훈련에 사용되었는지 여부를 추론하는 공격입니다.

**위협 내용**: 이 공격이 성공하면, 모델이 훈련 과정에서 사용자의 **민감한 개인 정보(예: 의료 기록, 위치 데이터)**를 암기했는지 여부를 파악할 수 있어, 개인정보 침해로 이어질 수 있습니다.

## 3. 서비스 거부 공격 (Availability Attacks)

모델이나 시스템의 접근성 및 가용성을 방해하여 정상적인 서비스 제공을 막는 공격입니다.

**위협 내용**: 일반적인 DDoS 공격과 유사하게, 과도한 컴퓨팅 자원을 요구하는 쿼리를 지속적으로 보내거나 모델 서버의 취약점을 공격하여, 모델이 정상적인 사용자에게 응답할 수 없도록 만듭니다.

## 4. 배포 및 환경 취약점 (Deployment & Environment Vulnerabilities)

AI 모델이 실행되거나 관리되는 인프라 단계에서의 보안 문제입니다.

- **API 취약점**: 모델에 접근하는 API 또는 인터페이스의 인증, 접근 제어, 입력 유효성 검사 등의 취약점을 악용하여 모델에 무단 접근하거나 오작동을 유도합니다.

- **공급망 공격 (Supply Chain Attacks)**: 모델 훈련에 사용되는 데이터셋, 라이브러리, 전이 학습된 모델(Pre-trained Model) 등에 악성 코드를 삽입하거나 백도어를 심어, AI 시스템 전체를 오염시킵니다.

- **환경 설정 오류**: 모델 서버의 운영 체제, 컨테이너 환경(Docker, Kubernetes), 클라우드 설정 등이 제대로 보안되지 않았을 때 발생하는 일반적인 보안 오류입니다.